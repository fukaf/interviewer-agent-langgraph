{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi-Agent Interview System using LangGraph\n",
    "============================================\n",
    "\n",
    "This system orchestrates multiple specialized agents to conduct deep technical interviews:\n",
    "- Initial Agent: Generates questions based on predefined topics\n",
    "- Security Agent: Validates answer relevance and quality\n",
    "- Judge Agent: Points out issues and asks for clarification\n",
    "- Topic Agent: Evaluates answer depth and completeness\n",
    "- Probe Agent: Generates follow-up questions for deeper exploration\n",
    "\"\"\"\n",
    "\n",
    "from typing import TypedDict, Literal, Annotated\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# STATE DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    \"\"\"State shared across all agents in the interview workflow\"\"\"\n",
    "    \n",
    "    # Topic management\n",
    "    topics: list[dict]  # List of topics from CSV\n",
    "    current_topic_index: int  # Index of current topic\n",
    "    current_topic: dict  # Current topic being discussed\n",
    "    topic_iteration_count: int  # Number of iterations for current topic\n",
    "    max_iterations_per_topic: int  # Maximum iterations allowed per topic\n",
    "    \n",
    "    # Question and answer tracking\n",
    "    current_question: str  # Current question being asked\n",
    "    user_answer: str  # User's response\n",
    "    \n",
    "    # Agent decisions\n",
    "    security_passed: bool  # Whether security check passed\n",
    "    security_feedback: str  # Feedback from security agent\n",
    "    topic_depth_sufficient: bool  # Whether answer is deep enough\n",
    "    topic_feedback: str  # Feedback from topic agent\n",
    "    \n",
    "    # Interview flow\n",
    "    interview_complete: bool  # Whether all topics are covered\n",
    "    conversation_history: list[dict]  # Full conversation log\n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# AGENT DEFINITIONS (Pseudo-code with structure)\n",
    "# ============================================================================\n",
    "\n",
    "def load_topics_from_csv(csv_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Load interview topics from CSV file\n",
    "    Expected CSV columns: topic, category, depth_level, key_points\n",
    "    \"\"\"\n",
    "    # TODO: Implement CSV loading\n",
    "    # df = pd.read_csv(csv_path)\n",
    "    # return df.to_dict('records')\n",
    "    \n",
    "    # Placeholder\n",
    "    return [\n",
    "        {\"topic\": \"Data Structures\", \"category\": \"Technical\", \"depth_level\": \"Advanced\", \"key_points\": \"Arrays, Trees, Graphs\"},\n",
    "        {\"topic\": \"System Design\", \"category\": \"Architecture\", \"depth_level\": \"Expert\", \"key_points\": \"Scalability, Reliability\"},\n",
    "        {\"topic\": \"Algorithms\", \"category\": \"Technical\", \"depth_level\": \"Advanced\", \"key_points\": \"Sorting, Searching, DP\"},\n",
    "    ]\n",
    "\n",
    "\n",
    "def initial_agent(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Initial Agent: Generates interview questions based on predefined topics\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Select next topic from the list\n",
    "    - Generate contextual question based on topic\n",
    "    - Initialize topic iteration counter\n",
    "    \"\"\"\n",
    "    # TODO: Implement actual LLM call\n",
    "    # model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "    \n",
    "    # Pseudo-code:\n",
    "    # if state[\"current_topic_index\"] >= len(state[\"topics\"]):\n",
    "    #     state[\"interview_complete\"] = True\n",
    "    #     return state\n",
    "    \n",
    "    # current_topic = state[\"topics\"][state[\"current_topic_index\"]]\n",
    "    # state[\"current_topic\"] = current_topic\n",
    "    \n",
    "    # prompt = f\"\"\"\n",
    "    # Generate an initial interview question for the topic: {current_topic['topic']}\n",
    "    # Category: {current_topic['category']}\n",
    "    # Key points to explore: {current_topic['key_points']}\n",
    "    # Make it conversational and engaging.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = model.invoke([SystemMessage(content=prompt)])\n",
    "    # state[\"current_question\"] = response.content\n",
    "    # state[\"topic_iteration_count\"] = 0\n",
    "    \n",
    "    print(\"üéØ INITIAL AGENT: Generating question for new topic...\")\n",
    "    state[\"current_question\"] = f\"Question about {state['current_topic']['topic']}\"\n",
    "    state[\"topic_iteration_count\"] = 0\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def security_agent(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Security Agent: Validates answer relevance and quality\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Check if answer is related to the question\n",
    "    - Validate answer quality (not too short, not off-topic)\n",
    "    - Set security_passed flag and provide feedback\n",
    "    \"\"\"\n",
    "    # TODO: Implement actual LLM call\n",
    "    # model = ChatOpenAI(model=\"gpt-4\", temperature=0.3)\n",
    "    \n",
    "    # Pseudo-code:\n",
    "    # prompt = f\"\"\"\n",
    "    # You are a security agent checking answer quality.\n",
    "    # \n",
    "    # Question: {state[\"current_question\"]}\n",
    "    # User Answer: {state[\"user_answer\"]}\n",
    "    # \n",
    "    # Evaluate:\n",
    "    # 1. Is the answer related to the question?\n",
    "    # 2. Is the answer substantive (not too short or vague)?\n",
    "    # 3. Is the answer on-topic?\n",
    "    # \n",
    "    # Respond with JSON:\n",
    "    # {{\n",
    "    #   \"passed\": true/false,\n",
    "    #   \"feedback\": \"explanation if failed\"\n",
    "    # }}\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = model.invoke([SystemMessage(content=prompt)])\n",
    "    # result = parse_json(response.content)\n",
    "    # state[\"security_passed\"] = result[\"passed\"]\n",
    "    # state[\"security_feedback\"] = result.get(\"feedback\", \"\")\n",
    "    \n",
    "    print(\"üîí SECURITY AGENT: Validating answer quality...\")\n",
    "    \n",
    "    # Simple pseudo validation\n",
    "    if len(state[\"user_answer\"]) < 10:\n",
    "        state[\"security_passed\"] = False\n",
    "        state[\"security_feedback\"] = \"Answer is too short\"\n",
    "    else:\n",
    "        state[\"security_passed\"] = True\n",
    "        state[\"security_feedback\"] = \"\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def judge_agent(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Judge Agent: Points out issues and requests clarification\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Analyze security feedback\n",
    "    - Generate helpful feedback to guide user\n",
    "    - Formulate request for better answer\n",
    "    \"\"\"\n",
    "    # TODO: Implement actual LLM call\n",
    "    # model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "    \n",
    "    # Pseudo-code:\n",
    "    # prompt = f\"\"\"\n",
    "    # You are a judge providing constructive feedback.\n",
    "    # \n",
    "    # Original Question: {state[\"current_question\"]}\n",
    "    # User Answer: {state[\"user_answer\"]}\n",
    "    # Issue: {state[\"security_feedback\"]}\n",
    "    # \n",
    "    # Provide friendly, constructive feedback and ask the user to try again.\n",
    "    # Be specific about what's missing or unclear.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = model.invoke([SystemMessage(content=prompt)])\n",
    "    # state[\"current_question\"] = response.content\n",
    "    \n",
    "    print(\"‚öñÔ∏è JUDGE AGENT: Providing feedback on answer quality...\")\n",
    "    state[\"current_question\"] = f\"I notice {state['security_feedback']}. Could you please elaborate?\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def topic_agent(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Topic Agent: Evaluates answer depth and completeness\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Assess if answer demonstrates sufficient depth\n",
    "    - Check coverage of key points for the topic\n",
    "    - Decide if topic is complete or needs more exploration\n",
    "    \"\"\"\n",
    "    # TODO: Implement actual LLM call\n",
    "    # model = ChatOpenAI(model=\"gpt-4\", temperature=0.3)\n",
    "    \n",
    "    # Pseudo-code:\n",
    "    # prompt = f\"\"\"\n",
    "    # You are evaluating answer depth for technical interviews.\n",
    "    # \n",
    "    # Topic: {state[\"current_topic\"][\"topic\"]}\n",
    "    # Expected Depth: {state[\"current_topic\"][\"depth_level\"]}\n",
    "    # Key Points: {state[\"current_topic\"][\"key_points\"]}\n",
    "    # \n",
    "    # Question Asked: {state[\"current_question\"]}\n",
    "    # User Answer: {state[\"user_answer\"]}\n",
    "    # \n",
    "    # Evaluate:\n",
    "    # 1. Does the answer demonstrate sufficient depth?\n",
    "    # 2. Are key concepts covered?\n",
    "    # 3. Should we probe deeper or move to next topic?\n",
    "    # \n",
    "    # Respond with JSON:\n",
    "    # {{\n",
    "    #   \"depth_sufficient\": true/false,\n",
    "    #   \"feedback\": \"what's covered or what's missing\"\n",
    "    # }}\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = model.invoke([SystemMessage(content=prompt)])\n",
    "    # result = parse_json(response.content)\n",
    "    # state[\"topic_depth_sufficient\"] = result[\"depth_sufficient\"]\n",
    "    # state[\"topic_feedback\"] = result[\"feedback\"]\n",
    "    \n",
    "    print(\"üìä TOPIC AGENT: Evaluating answer depth...\")\n",
    "    \n",
    "    # Simple pseudo evaluation\n",
    "    state[\"topic_iteration_count\"] += 1\n",
    "    \n",
    "    if state[\"topic_iteration_count\"] >= state[\"max_iterations_per_topic\"]:\n",
    "        state[\"topic_depth_sufficient\"] = True\n",
    "        state[\"topic_feedback\"] = \"Max iterations reached\"\n",
    "    else:\n",
    "        # Randomly decide for demo\n",
    "        state[\"topic_depth_sufficient\"] = len(state[\"user_answer\"]) > 50\n",
    "        state[\"topic_feedback\"] = \"Need more detail\" if not state[\"topic_depth_sufficient\"] else \"Good depth\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def probe_agent(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Probe Agent: Generates follow-up questions for deeper exploration\n",
    "    \n",
    "    Responsibilities:\n",
    "    - Analyze user's answer to identify areas to probe\n",
    "    - Generate specific follow-up questions\n",
    "    - Focus on uncovered key points or shallow areas\n",
    "    \"\"\"\n",
    "    # TODO: Implement actual LLM call\n",
    "    # model = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "    \n",
    "    # Pseudo-code:\n",
    "    # prompt = f\"\"\"\n",
    "    # You are a probe agent conducting deep technical interviews.\n",
    "    # \n",
    "    # Topic: {state[\"current_topic\"][\"topic\"]}\n",
    "    # Key Points to Cover: {state[\"current_topic\"][\"key_points\"]}\n",
    "    # Previous Question: {state[\"current_question\"]}\n",
    "    # User Answer: {state[\"user_answer\"]}\n",
    "    # Topic Feedback: {state[\"topic_feedback\"]}\n",
    "    # \n",
    "    # Generate a follow-up question that:\n",
    "    # 1. Digs deeper into the user's answer\n",
    "    # 2. Explores uncovered key points\n",
    "    # 3. Reveals practical understanding\n",
    "    # \n",
    "    # Keep it conversational and specific to their answer.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # response = model.invoke([SystemMessage(content=prompt)])\n",
    "    # state[\"current_question\"] = response.content\n",
    "    \n",
    "    print(\"üîç PROBE AGENT: Generating follow-up question...\")\n",
    "    state[\"current_question\"] = f\"Follow-up: Can you elaborate on {state['current_topic']['topic']}?\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def move_to_next_topic(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Helper function to advance to the next topic\n",
    "    \"\"\"\n",
    "    print(\"‚úÖ MOVING TO NEXT TOPIC...\")\n",
    "    state[\"current_topic_index\"] += 1\n",
    "    \n",
    "    if state[\"current_topic_index\"] >= len(state[\"topics\"]):\n",
    "        state[\"interview_complete\"] = True\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ROUTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def route_after_security(state: InterviewState) -> Literal[\"judge\", \"topic_agent\"]:\n",
    "    \"\"\"Route based on security check results\"\"\"\n",
    "    if state[\"security_passed\"]:\n",
    "        return \"topic_agent\"\n",
    "    else:\n",
    "        return \"judge\"\n",
    "\n",
    "\n",
    "def route_after_topic(state: InterviewState) -> Literal[\"initial_agent\", \"probe_agent\", \"end\"]:\n",
    "    \"\"\"Route based on topic depth evaluation\"\"\"\n",
    "    # Check if max iterations reached\n",
    "    if state[\"topic_iteration_count\"] >= state[\"max_iterations_per_topic\"]:\n",
    "        # Move to next topic\n",
    "        if state[\"current_topic_index\"] + 1 >= len(state[\"topics\"]):\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"initial_agent\"\n",
    "    \n",
    "    # Check if depth is sufficient\n",
    "    if state[\"topic_depth_sufficient\"]:\n",
    "        # Move to next topic\n",
    "        if state[\"current_topic_index\"] + 1 >= len(state[\"topics\"]):\n",
    "            return \"end\"\n",
    "        else:\n",
    "            return \"initial_agent\"\n",
    "    else:\n",
    "        return \"probe_agent\"\n",
    "\n",
    "\n",
    "def should_end_interview(state: InterviewState) -> Literal[\"end\", \"continue\"]:\n",
    "    \"\"\"Check if interview should end\"\"\"\n",
    "    if state[\"interview_complete\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPH CONSTRUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_interview_graph():\n",
    "    \"\"\"\n",
    "    Construct the multi-agent interview workflow graph\n",
    "    \n",
    "    Flow:\n",
    "    1. Initial Agent ‚Üí generates question\n",
    "    2. User answers (external input)\n",
    "    3. Security Agent ‚Üí validates answer\n",
    "    4. If failed ‚Üí Judge Agent ‚Üí back to user (via Security Agent)\n",
    "    5. If passed ‚Üí Topic Agent ‚Üí evaluates depth\n",
    "    6. If sufficient depth ‚Üí next topic (back to Initial Agent)\n",
    "    7. If insufficient depth ‚Üí Probe Agent ‚Üí generates follow-up\n",
    "    8. Probe answer ‚Üí back to Security Agent\n",
    "    9. Repeat until all topics covered or iteration limits reached\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(InterviewState)\n",
    "    \n",
    "    # Add nodes (agents)\n",
    "    workflow.add_node(\"initial_agent\", initial_agent)\n",
    "    workflow.add_node(\"security_agent\", security_agent)\n",
    "    workflow.add_node(\"judge\", judge_agent)\n",
    "    workflow.add_node(\"topic_agent\", topic_agent)\n",
    "    workflow.add_node(\"probe_agent\", probe_agent)\n",
    "    workflow.add_node(\"next_topic\", move_to_next_topic)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.add_edge(START, \"initial_agent\")\n",
    "    \n",
    "    # Add edges\n",
    "    # Initial agent ‚Üí wait for user input ‚Üí security agent\n",
    "    workflow.add_edge(\"initial_agent\", \"security_agent\")\n",
    "    \n",
    "    # Security agent ‚Üí conditional routing\n",
    "    workflow.add_conditional_edges(\n",
    "        \"security_agent\",\n",
    "        route_after_security,\n",
    "        {\n",
    "            \"judge\": \"judge\",\n",
    "            \"topic_agent\": \"topic_agent\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Judge ‚Üí back to security (user will answer again)\n",
    "    workflow.add_edge(\"judge\", \"security_agent\")\n",
    "    \n",
    "    # Topic agent ‚Üí conditional routing\n",
    "    workflow.add_conditional_edges(\n",
    "        \"topic_agent\",\n",
    "        route_after_topic,\n",
    "        {\n",
    "            \"initial_agent\": \"next_topic\",\n",
    "            \"probe_agent\": \"probe_agent\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Next topic ‚Üí back to initial agent\n",
    "    workflow.add_edge(\"next_topic\", \"initial_agent\")\n",
    "    \n",
    "    # Probe agent ‚Üí back to security (user will answer probe question)\n",
    "    workflow.add_edge(\"probe_agent\", \"security_agent\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    graph = workflow.compile()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def display_graph():\n",
    "    \"\"\"\n",
    "    Display the interview workflow graph\n",
    "    \"\"\"\n",
    "    graph = create_interview_graph()\n",
    "    \n",
    "    try:\n",
    "        # Try to display using IPython/Jupyter\n",
    "        from IPython.display import Image, display as ipython_display\n",
    "        \n",
    "        # Generate graph visualization\n",
    "        graph_image = graph.get_graph().draw_mermaid_png()\n",
    "        ipython_display(Image(graph_image))\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"IPython not available. Displaying Mermaid diagram code instead:\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        mermaid_code = graph.get_graph().draw_mermaid()\n",
    "        print(mermaid_code)\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(\"Copy the above code to https://mermaid.live to visualize the graph\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_interview_simulation():\n",
    "    \"\"\"\n",
    "    Simulate an interview session (for testing)\n",
    "    \"\"\"\n",
    "    # Initialize state\n",
    "    initial_state = {\n",
    "        \"topics\": load_topics_from_csv(\"topics.csv\"),\n",
    "        \"current_topic_index\": 0,\n",
    "        \"current_topic\": {},\n",
    "        \"topic_iteration_count\": 0,\n",
    "        \"max_iterations_per_topic\": 3,\n",
    "        \"current_question\": \"\",\n",
    "        \"user_answer\": \"\",\n",
    "        \"security_passed\": False,\n",
    "        \"security_feedback\": \"\",\n",
    "        \"topic_depth_sufficient\": False,\n",
    "        \"topic_feedback\": \"\",\n",
    "        \"interview_complete\": False,\n",
    "        \"conversation_history\": []\n",
    "    }\n",
    "    \n",
    "    # Create graph\n",
    "    graph = create_interview_graph()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MULTI-AGENT INTERVIEW SYSTEM - SIMULATION\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # TODO: Implement actual interview loop\n",
    "    # This would involve:\n",
    "    # 1. Running the graph step by step\n",
    "    # 2. Getting user input when needed\n",
    "    # 3. Updating state with user responses\n",
    "    # 4. Continuing until interview_complete = True\n",
    "    \n",
    "    print(\"Graph created successfully!\")\n",
    "    print(f\"Loaded {len(initial_state['topics'])} topics\")\n",
    "    print(\"\\nTo run the interview, integrate with streamlit_app.py\")\n",
    "    \n",
    "    return graph, initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Display the graph structure\n",
    "    print(\"Generating multi-agent interview workflow graph...\\n\")\n",
    "    graph = display_graph()\n",
    "    \n",
    "    print(\"\\n‚úÖ Graph created successfully!\")\n",
    "    print(\"\\nAgent Roles:\")\n",
    "    print(\"  üéØ Initial Agent: Generates questions from topics\")\n",
    "    print(\"  üîí Security Agent: Validates answer relevance & quality\")\n",
    "    print(\"  ‚öñÔ∏è Judge Agent: Provides feedback on failed answers\")\n",
    "    print(\"  üìä Topic Agent: Evaluates answer depth\")\n",
    "    print(\"  üîç Probe Agent: Generates follow-up questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463451b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea7c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb881a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07075f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
